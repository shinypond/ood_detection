{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "\n",
    "class config:\n",
    "    dataroot = 'data'\n",
    "    workers = 2\n",
    "    imageSize = 32\n",
    "    nc = 1 # input image channels\n",
    "    nz = 100 # size of the latent z vector\n",
    "    ngf = 32\n",
    "    ngpu = 1\n",
    "    state_E = '../../saved_models/VAE_fmnist/netE_pixel.pth'#'../saved_models/models/fmnist/netE.pth'\n",
    "    state_G = '../../saved_models/VAE_cifar10/netG_pixel.pth'#'../saved_models/models/fmnist/netG.pth'\n",
    "    batch_size = 1\n",
    "    train_batchsize = 1\n",
    "    num_samples=20\n",
    "    beta = 1\n",
    "opt = config\n",
    "\n",
    "np.random.seed(1234)\n",
    "torch.manual_seed(1234)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.set_deterministic(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for DCGAN_G:\n\tMissing key(s) in state_dict: \"main.initial:100-128:convt.weight\", \"main.initial:128:batchnorm.weight\", \"main.initial:128:batchnorm.bias\", \"main.initial:128:batchnorm.running_mean\", \"main.initial:128:batchnorm.running_var\", \"main.pyramid:64-32:convt.weight\", \"main.pyramid:32:batchnorm.weight\", \"main.pyramid:32:batchnorm.bias\", \"main.pyramid:32:batchnorm.running_mean\", \"main.pyramid:32:batchnorm.running_var\", \"main.final:32-1:convt.weight\". \n\tUnexpected key(s) in state_dict: \"main.initial:100-256:convt.weight\", \"main.initial:256:batchnorm.weight\", \"main.initial:256:batchnorm.bias\", \"main.initial:256:batchnorm.running_mean\", \"main.initial:256:batchnorm.running_var\", \"main.initial:256:batchnorm.num_batches_tracked\", \"main.pyramid:256-128:convt.weight\", \"main.pyramid:128:batchnorm.weight\", \"main.pyramid:128:batchnorm.bias\", \"main.pyramid:128:batchnorm.running_mean\", \"main.pyramid:128:batchnorm.running_var\", \"main.pyramid:128:batchnorm.num_batches_tracked\", \"main.final:64-3:convt.weight\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-0763000e8a10>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mstate_G\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_G\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_location\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mnetG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_G\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mnetE\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDVAE\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimageSize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mngf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mngpu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ycy\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m   1049\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1050\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1051\u001b[1;33m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[0;32m   1052\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0;32m   1053\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for DCGAN_G:\n\tMissing key(s) in state_dict: \"main.initial:100-128:convt.weight\", \"main.initial:128:batchnorm.weight\", \"main.initial:128:batchnorm.bias\", \"main.initial:128:batchnorm.running_mean\", \"main.initial:128:batchnorm.running_var\", \"main.pyramid:64-32:convt.weight\", \"main.pyramid:32:batchnorm.weight\", \"main.pyramid:32:batchnorm.bias\", \"main.pyramid:32:batchnorm.running_mean\", \"main.pyramid:32:batchnorm.running_var\", \"main.final:32-1:convt.weight\". \n\tUnexpected key(s) in state_dict: \"main.initial:100-256:convt.weight\", \"main.initial:256:batchnorm.weight\", \"main.initial:256:batchnorm.bias\", \"main.initial:256:batchnorm.running_mean\", \"main.initial:256:batchnorm.running_var\", \"main.initial:256:batchnorm.num_batches_tracked\", \"main.pyramid:256-128:convt.weight\", \"main.pyramid:128:batchnorm.weight\", \"main.pyramid:128:batchnorm.bias\", \"main.pyramid:128:batchnorm.running_mean\", \"main.pyramid:128:batchnorm.running_var\", \"main.pyramid:128:batchnorm.num_batches_tracked\", \"main.final:64-3:convt.weight\". "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import DCGAN_VAE_pixel as DVAE\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "netG = DVAE.DCGAN_G(opt.imageSize, opt.nz, opt.nc, opt.ngf, opt.ngpu)\n",
    "\n",
    "state_G = torch.load(opt.state_G, map_location = device)\n",
    "netG.load_state_dict(state_G)\n",
    "\n",
    "netE = DVAE.Encoder(opt.imageSize, opt.nz, opt.nc, opt.ngf, opt.ngpu)\n",
    "state_E = torch.load(opt.state_E, map_location = device)\n",
    "netE.load_state_dict(state_E)\n",
    "\n",
    "netG.to(device)\n",
    "netE.to(device)\n",
    "netE.eval()\n",
    "netG.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(128, 100, kernel_size=(4, 4), stride=(1, 1))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netE.conv1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fmnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "dataset_fmnist = dset.FashionMNIST(root=opt.dataroot, train=False, download=True, transform=transforms.Compose([\n",
    "                                transforms.Resize((opt.imageSize)),\n",
    "                                transforms.ToTensor(),]))\n",
    "test_loader_fmnist = torch.utils.data.DataLoader(dataset_fmnist, batch_size=2,\n",
    "                                            shuffle=False, num_workers=int(opt.workers))\n",
    "    \n",
    "dataset_mnist = dset.MNIST(root=opt.dataroot, train=False, download=True, transform=transforms.Compose([\n",
    "                                transforms.Resize((opt.imageSize,opt.imageSize)),\n",
    "                                transforms.ToTensor(),\n",
    "                            ]))\n",
    "\n",
    "test_loader_mnist = torch.utils.data.DataLoader(dataset_mnist, batch_size=opt.batch_size,\n",
    "                                            shuffle=False, num_workers=int(opt.workers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate Fisher Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_fmnist_train = dset.FashionMNIST(root=opt.dataroot, train=True, download=True, transform=transforms.Compose([\n",
    "                                transforms.Resize((opt.imageSize)),\n",
    "                                transforms.ToTensor(),\n",
    "                            ]))\n",
    "dataloader_fmnist = torch.utils.data.DataLoader(dataset_fmnist_train, batch_size=opt.train_batchsize,\n",
    "                                                shuffle=True, num_workers=int(opt.workers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "\n",
    "def KL_div(mu,logvar,reduction = 'avg'):\n",
    "    mu = mu.view(mu.size(0),mu.size(1))\n",
    "    logvar = logvar.view(logvar.size(0), logvar.size(1))\n",
    "    if reduction == 'sum':\n",
    "        return -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) \n",
    "    else:\n",
    "        KL = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp(),1) \n",
    "        return KL\n",
    "\n",
    "def Calculate_fisher(dataloader, dicts, max_iter, num_samples):\n",
    "    optimizer1 = optim.SGD(netE.parameters(), lr=0,momentum=0)\n",
    "    optimizer2 = optim.SGD(netG.parameters(), lr=0,momentum=0)\n",
    "    loss_fn = nn.CrossEntropyLoss(reduction = 'none')\n",
    "    for i, (x, _) in enumerate(dataloader):\n",
    "\n",
    "        optimizer1.zero_grad()\n",
    "        optimizer2.zero_grad()\n",
    "        x = x.repeat(num_samples,1,1,1).to(device)\n",
    "        b = x.size(0)\n",
    "        target = Variable(x.data.view(-1) * 255).long()\n",
    "        [z,mu,logvar] = netE(x)\n",
    "        kld = KL_div(mu,logvar)\n",
    "        recon = netG(z)\n",
    "        recon = recon.contiguous()\n",
    "        recon = recon.view(-1,256)\n",
    "        recl = loss_fn(recon, target)\n",
    "        recl = torch.sum(recl) / b\n",
    "        loss =  recl + kld.mean()\n",
    "        grads = []\n",
    "        loss.backward(retain_graph=True)\n",
    "        \n",
    "        for x in dicts:\n",
    "            for param in x.parameters():\n",
    "                grads.append(param.grad.view(-1)**2) \n",
    "        grads = torch.cat(grads)\n",
    "        if i!=0:\n",
    "            Grads = (i*Grads + grads)/(i+1)\n",
    "            if i>max_iter:\n",
    "                break\n",
    "        else:\n",
    "            Grads = grads\n",
    "\n",
    "    Grads = torch.sqrt(Grads)\n",
    "    Grads = Grads*(Grads>1e-3)\n",
    "    Grads[Grads==0] = 1e-3\n",
    "    normalize_factor = 2*np.sqrt(len(Grads))\n",
    "    return Grads, normalize_factor\n",
    "\n",
    "def Calculate_score(dataloader, dicts, Grads, normalize_factor, number, num_samples):\n",
    "    optimizer1 = optim.SGD(netE.parameters(), lr=0,momentum=0)\n",
    "    optimizer2 = optim.SGD(netG.parameters(), lr=0,momentum=0)\n",
    "    loss_fn = nn.CrossEntropyLoss(reduction = 'none')\n",
    "    score = []\n",
    "    for i, (x, _) in enumerate(dataloader):\n",
    "        optimizer1.zero_grad()\n",
    "        optimizer2.zero_grad()\n",
    "        x = x.repeat(num_samples,1,1,1).to(device)\n",
    "        b = x.size(0)\n",
    "        target = Variable(x.data.view(-1) * 255).long()\n",
    "        gradient_val=0\n",
    "        [z,mu,logvar] = netE(x)\n",
    "        kld = KL_div(mu,logvar)\n",
    "        recon = netG(z)\n",
    "        recon = recon.contiguous()\n",
    "        recon = recon.view(-1,256)\n",
    "        recl = loss_fn(recon, target)\n",
    "        loss =  torch.sum(recl)/b + kld.mean()\n",
    "        loss.backward(retain_graph=True)\n",
    "        grads = []\n",
    "        for x in dicts:\n",
    "            for param in x.parameters():\n",
    "                grads.append(param.grad.view(-1)) \n",
    "        grads = torch.cat(grads)\n",
    "        gradient_val = torch.norm(grads/Grads)/normalize_factor\n",
    "        score.append(gradient_val.detach().cpu())\n",
    "        if i%number==number-1:\n",
    "            break\n",
    "    return score\n",
    "\n",
    "def plot_hist(indist_Grads, outdist_Grads):\n",
    "    indist_Grads = torch.tensor(indist_Grads)\n",
    "    plt.hist(indist_Grads, bins=100)\n",
    "\n",
    "    outdist_Grads = torch.tensor(outdist_Grads)\n",
    "    plt.hist(outdist_Grads, bins=500, alpha=0.5)\n",
    "    plt.xlim(0,10)\n",
    "\n",
    "\n",
    "def AUROC(indist_Grads, outdist_Grads):    \n",
    "    combined = np.concatenate((indist_Grads, outdist_Grads))\n",
    "    label_1 = np.ones(len(indist_Grads))\n",
    "    label_2 = np.zeros(len(outdist_Grads))\n",
    "    label = np.concatenate((label_1, label_2))\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(label, combined, pos_label=0)\n",
    "    #plot_roc_curve(fpr, tpr)\n",
    "    rocauc = metrics.auc(fpr, tpr)\n",
    "    print('AUC for Gradient Norm is: {}'.format(rocauc))\n",
    "    plt.plot(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicts = [netG.main[-1],netE.conv1]\n",
    "Grads, normalize_factor = Calculate_fisher(dataloader_fmnist, dicts, max_iter=10000, num_samples=opt.num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmnist_Gradients = Calculate_score(test_loader_fmnist, dicts, Grads, normalize_factor, 2000, opt.num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_Gradients = Calculate_score(test_loader_mnist, dicts, Grads, normalize_factor, 2000, opt.num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_hist(fmnist_Gradients, mnist_Gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUROC(fmnist_Gradients, mnist_Gradients)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ycy",
   "language": "python",
   "name": "ycy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
