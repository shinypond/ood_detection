{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import torch\n",
    "from models import *\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import torch.utils.data as data\n",
    "import torchvision.datasets as dset\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "def plot_hist(*args, bins=[100, 100], labels=['cifar10', 'svhn'], xlim=[0, 10]):\n",
    "    \n",
    "    if len(args) != len(bins) or len(args) != len(labels) or len(bins) != len(labels):\n",
    "        print('Oops! GRADS, BINS and LABELS must have the same length !')\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    fig = plt.figure(figsize=(12, 9))\n",
    "    \n",
    "    in_dist_Grads = torch.tensor(args[0]).detach().cpu().numpy()\n",
    "    plt.hist(in_dist_Grads, bins=bins[0], density=True, alpha=0.9, color='black', label=labels[0])\n",
    "\n",
    "    for i in range(1, len(args)):\n",
    "        out_dist_Grads = torch.tensor(args[i]).detach().cpu().numpy()\n",
    "        plt.hist(out_dist_Grads, bins=bins[i], density=True, alpha=0.5, label=labels[i])\n",
    "    \n",
    "    plt.xlim(xlim[0], xlim[1])\n",
    "    plt.title(f'In-Dist : {labels[0]}  /  Out-Dist : {[labels[i] for i in range(1, len(args))]}')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def AUROC(*args, labels=['cifar10', 'svhn'], plot=True):    \n",
    "    in_dist_Grads = args[0]\n",
    "    out_dist_Grads = args[1]\n",
    "    combined = np.concatenate((in_dist_Grads, out_dist_Grads))\n",
    "    label_1 = np.ones(len(in_dist_Grads))\n",
    "    label_2 = np.zeros(len(out_dist_Grads))\n",
    "    label = np.concatenate((label_1, label_2))\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(label, combined, pos_label=0)\n",
    "    #plot_roc_curve(fpr, tpr)\n",
    "    rocauc = metrics.auc(fpr, tpr)\n",
    "    title = f'In-dist : {labels[0]}  /  Out-dist : {labels[1]} \\n AUC for Gradient Norm is: {rocauc:.6f}'\n",
    "    if plot:\n",
    "        fig = plt.figure(figsize=(12, 9))\n",
    "        plt.plot(fpr, tpr)\n",
    "        plt.title(title)\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    return rocauc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OPT():\n",
    "    def __init__(self, train_dist):\n",
    "        self.dataroot = '../data'\n",
    "        self.imageSize = 32\n",
    "        self.workers = 0\n",
    "        if train_dist == 'cifar10':\n",
    "            self.nc = 3\n",
    "        elif train_dist == 'fmnist':\n",
    "            self.nc = 1\n",
    "\n",
    "\n",
    "def TEST_loader(train_dist, target_dist, batch_size=10, shuffle=False):\n",
    "    \n",
    "    \"\"\" Return test_loader for given 'train_dist' and 'target_dist' \"\"\"\n",
    "    \n",
    "    \"\"\" train_dist = 'cifar10' or 'fmnist' \"\"\"\n",
    "    \n",
    "    \"\"\" target_dist (In-Distribution or Out-of-Distribution)\n",
    "    \n",
    "            if train_dist is 'cifar10' (train), target_dist should be one of\n",
    "                    - cifar10 (test)\n",
    "                    - svhn (test)     \n",
    "                    - celeba (test)   \n",
    "                    - lsun (test)     \n",
    "                    - cifar100 (test) \n",
    "                    - mnist (test)    \n",
    "                    - fmnist (test)   \n",
    "                    - kmnist (test)   \n",
    "                    - omniglot (eval) \n",
    "                    - notmnist (small)\n",
    "                    - trafficsign\n",
    "                    - noise\n",
    "                    - constant\n",
    "            \n",
    "            if train_dist is 'fmnist' (train), target_dist should be one of\n",
    "                    - fmnist (test)\n",
    "                    - svhn (test)     \n",
    "                    - celeba (test)   \n",
    "                    - lsun (test)     \n",
    "                    - cifar10 (test)  \n",
    "                    - cifar100 (test) \n",
    "                    - mnist (test)    \n",
    "                    - kmnist (test)   \n",
    "                    - omniglot (eval) \n",
    "                    - notmnist (small)\n",
    "                    - noise\n",
    "                    - constant\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    preprocess1 = [transforms.Normalize((0.48,), (0.2,))]\n",
    "    preprocess3 = [transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))]\n",
    "    \n",
    "    if train_dist == 'cifar10':\n",
    "        opt = OPT('cifar10')\n",
    "\n",
    "        if target_dist == 'cifar10':\n",
    "            return test_loader_cifar10(opt, preprocess3, batch_size, shuffle)\n",
    "\n",
    "        elif target_dist == 'svhn':\n",
    "            return test_loader_svhn(opt, preprocess3, batch_size, shuffle)\n",
    "\n",
    "        elif target_dist == 'celeba':\n",
    "            return test_loader_celeba(opt, preprocess3, batch_size, shuffle)\n",
    "        \n",
    "        elif target_dist == 'lsun':\n",
    "            return test_loader_lsun(opt, preprocess3, batch_size, shuffle)\n",
    "        \n",
    "        elif target_dist == 'cifar100':\n",
    "            return test_loader_cifar100(opt, preprocess3, batch_size, shuffle)\n",
    "        \n",
    "        elif target_dist == 'mnist':\n",
    "            return test_loader_mnist(opt, preprocess3, batch_size, shuffle)\n",
    "        \n",
    "        elif target_dist == 'fmnist':\n",
    "            return test_loader_fmnist(opt, preprocess3, batch_size, shuffle)\n",
    "        \n",
    "        elif target_dist == 'kmnist':\n",
    "            return test_loader_kmnist(opt, preprocess3, batch_size, shuffle)\n",
    "        \n",
    "        elif target_dist == 'omniglot':\n",
    "            return test_loader_omniglot(opt, preprocess3, batch_size, shuffle)\n",
    "        \n",
    "        elif target_dist == 'notmnist':\n",
    "            return test_loader_notmnist(opt, preprocess3, batch_size, shuffle)\n",
    "        \n",
    "        elif target_dist == 'trafficsign':\n",
    "            return test_loader_trafficsign(opt, preprocess3, batch_size, shuffle)\n",
    "        \n",
    "        elif target_dist == 'noise':\n",
    "            return test_loader_noise(opt, preprocess3, batch_size, shuffle)\n",
    "        \n",
    "        elif target_dist == 'constant':\n",
    "            return test_loader_constant(opt, preprocess3, batch_size, shuffle)\n",
    "        \n",
    "        else:\n",
    "            raise NotImplementedError(\"Oops! Such match of ID & OOD doesn't exist!\")\n",
    "\n",
    "    elif train_dist == 'fmnist':\n",
    "        opt = OPT('fmnist')\n",
    "\n",
    "        if target_dist == 'fmnist':\n",
    "            return test_loader_fmnist(opt, preprocess1, batch_size, shuffle)\n",
    "            \n",
    "        elif target_dist == 'svhn':\n",
    "            return test_loader_svhn(opt, preprocess1, batch_size, shuffle)\n",
    "        \n",
    "        elif target_dist == 'celeba':\n",
    "            return test_loader_celeba(opt, preprocess1, batch_size, shuffle)\n",
    "        \n",
    "        elif target_dist == 'lsun':\n",
    "            return test_loader_lsun(opt, preprocess1, batch_size, shuffle)\n",
    "        \n",
    "        elif target_dist == 'cifar10':\n",
    "            return test_loader_cifar10(opt, preprocess1, batch_size, shuffle)\n",
    "        \n",
    "        elif target_dist == 'cifar100':\n",
    "            return test_loader_cifar100(opt, preprocess1, batch_size, shuffle)\n",
    "        \n",
    "        elif target_dist == 'mnist':\n",
    "            return test_loader_mnist(opt, preprocess1, batch_size, shuffle)\n",
    "        \n",
    "        elif target_dist == 'kmnist':\n",
    "            return test_loader_kmnist(opt, preprocess1, batch_size, shuffle)\n",
    "        \n",
    "        elif target_dist == 'omniglot':\n",
    "            return test_loader_omniglot(opt, preprocess1, batch_size, shuffle)\n",
    "            \n",
    "        elif target_dist == 'notmnist':\n",
    "            return test_loader_notmnist(opt, preprocess1, batch_size, shuffle)\n",
    "            \n",
    "        elif target_dist == 'noise':\n",
    "            return test_loader_noise(opt, preprocess1, batch_size, shuffle)\n",
    "        \n",
    "        elif target_dist == 'constant':\n",
    "            return test_loader_constant(opt, preprocess1, batch_size, shuffle)\n",
    "        \n",
    "        else:\n",
    "            raise NotImplementedError(\"Oops! Such match of ID & OOD doesn't exist!\")\n",
    "        \n",
    "    else:\n",
    "        raise NotImplementedError(\"Oops! Such match of ID & OOD doesn't exist!\")\n",
    "        \n",
    "\n",
    "def rgb_to_gray(x):\n",
    "    return torch.mean(x, dim=0, keepdim=True)\n",
    "\n",
    "\n",
    "def gray_to_rgb(x):\n",
    "    return x.repeat(3, 1, 1)\n",
    "\n",
    "\n",
    "def test_loader_cifar10(opt, preprocess, batch_size, shuffle):\n",
    "    if opt.nc == 1:\n",
    "        preprocess = [rgb_to_gray] + preprocess\n",
    "    dataset_cifar10 = dset.CIFAR10(\n",
    "        root=opt.dataroot,\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=transforms.Compose([\n",
    "            transforms.Resize((opt.imageSize)),\n",
    "            transforms.ToTensor(),\n",
    "        ] + preprocess),\n",
    "    )\n",
    "    test_loader_cifar10 = data.DataLoader(\n",
    "        dataset_cifar10,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=int(opt.workers),\n",
    "    )\n",
    "    return test_loader_cifar10\n",
    "\n",
    "\n",
    "def test_loader_svhn(opt, preprocess, batch_size, shuffle):\n",
    "    if opt.nc == 1:\n",
    "        preprocess = [rgb_to_gray] + preprocess\n",
    "    dataset_svhn = dset.SVHN(\n",
    "        root=opt.dataroot,\n",
    "        split='test',\n",
    "        download=True,\n",
    "        transform=transforms.Compose([\n",
    "            transforms.Resize((opt.imageSize,opt.imageSize)),\n",
    "            transforms.ToTensor(),\n",
    "        ] + preprocess),\n",
    "    )\n",
    "    test_loader_svhn = data.DataLoader(\n",
    "        dataset_svhn,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=int(opt.workers),\n",
    "    )\n",
    "    return test_loader_svhn\n",
    "\n",
    "    \n",
    "def test_loader_celeba(opt, preprocess, batch_size, shuffle):\n",
    "    if opt.nc == 1:\n",
    "        preprocess = [rgb_to_gray] + preprocess\n",
    "    class CelebA(data.Dataset):\n",
    "        def __init__(self, db_path, transform=None):\n",
    "            super(CelebA, self).__init__()\n",
    "            self.db_path = db_path\n",
    "            elements = os.listdir(self.db_path)\n",
    "            self.total_path = [self.db_path + '/' + element for element in elements]\n",
    "            self.transform = transform\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.total_path)\n",
    "\n",
    "        def __getitem__(self, index):\n",
    "            current_path = self.total_path[index]\n",
    "            img = cv2.imread(current_path)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            img = Image.fromarray(img)\n",
    "            img = self.transform(img)\n",
    "            return img\n",
    "\n",
    "    transform=transforms.Compose([\n",
    "        transforms.Resize((opt.imageSize, opt.imageSize)),\n",
    "        transforms.ToTensor(),\n",
    "    ] + preprocess)\n",
    "\n",
    "    celeba = CelebA('../../data/celeba/archive', transform=transform)\n",
    "    test_loader_celeba = data.DataLoader(\n",
    "        celeba,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=0,\n",
    "    )\n",
    "    return test_loader_celeba\n",
    "\n",
    "\n",
    "def test_loader_lsun(opt, preprocess, batch_size, shuffle):\n",
    "    if opt.nc == 1:\n",
    "        preprocess = [rgb_to_gray] + preprocess\n",
    "    class LSUN(data.Dataset):\n",
    "        def __init__(self, db_path, categories=['bedroom', 'bridge', 'church_outdoor', 'classroom', 'conference_room', 'dining_room', 'kitchen', 'living_room', 'restaurant', 'tower'], transform=None):\n",
    "            super(LSUN, self).__init__()\n",
    "            self.total_path = []\n",
    "            for i in range(len(categories)):\n",
    "                self.db_path = db_path + '/' + categories[i] + '_val'\n",
    "                elements = os.listdir(self.db_path)\n",
    "                self.total_path += [self.db_path + '/' + element for element in elements if element[-4:] == '.jpg']\n",
    "            self.transform = transform\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.total_path)\n",
    "\n",
    "        def __getitem__(self, index):\n",
    "            current_path = self.total_path[index]\n",
    "            img = cv2.imread(current_path)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            img = Image.fromarray(img)\n",
    "            img = self.transform(img)\n",
    "            return img\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "       # transforms.Resize(opt.imageSize), # Then the size will be H x 32 or 32 x W (32 is smaller)\n",
    "        transforms.CenterCrop(opt.imageSize),\n",
    "        transforms.ToTensor(),\n",
    "    ] + preprocess)\n",
    "\n",
    "    lsun = LSUN('../../data/LSUN_test', transform=transform)\n",
    "    test_loader_lsun = data.DataLoader(\n",
    "        lsun,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=0,\n",
    "    )\n",
    "    return test_loader_lsun\n",
    "\n",
    "\n",
    "def test_loader_cifar100(opt, preprocess, batch_size, shuffle):\n",
    "    if opt.nc == 1:\n",
    "        preprocess = [rgb_to_gray] + preprocess\n",
    "    dataset_cifar100 = dset.CIFAR100(\n",
    "        root=opt.dataroot,\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=transforms.Compose([\n",
    "            transforms.Resize((opt.imageSize)),\n",
    "            transforms.ToTensor(),\n",
    "        ] + preprocess),\n",
    "    )\n",
    "    test_loader_cifar100 = data.DataLoader(\n",
    "        dataset_cifar100,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=int(opt.workers),\n",
    "    )\n",
    "    return test_loader_cifar100\n",
    "\n",
    "\n",
    "def test_loader_mnist(opt, preprocess, batch_size, shuffle):\n",
    "    if opt.nc == 3:\n",
    "        preprocess = [gray_to_rgb] + preprocess\n",
    "    dataset_mnist = dset.MNIST(\n",
    "        root=opt.dataroot,\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=transforms.Compose([\n",
    "            transforms.Resize((opt.imageSize, opt.imageSize)),\n",
    "            transforms.ToTensor(),\n",
    "        ] + preprocess),\n",
    "    )\n",
    "    test_loader_mnist = data.DataLoader(\n",
    "        dataset_mnist,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=int(opt.workers),\n",
    "    )\n",
    "    return test_loader_mnist\n",
    "\n",
    "    \n",
    "def test_loader_fmnist(opt, preprocess, batch_size, shuffle):\n",
    "    if opt.nc == 3:\n",
    "        preprocess = [gray_to_rgb] + preprocess\n",
    "    dataset_fmnist = dset.FashionMNIST(\n",
    "        root=opt.dataroot,\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=transforms.Compose([\n",
    "            transforms.Resize((opt.imageSize)),\n",
    "            transforms.ToTensor(),\n",
    "        ] + preprocess),\n",
    "    )\n",
    "    test_loader_fmnist = data.DataLoader(\n",
    "        dataset_fmnist,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=int(opt.workers),\n",
    "    )\n",
    "    return test_loader_fmnist\n",
    "\n",
    "    \n",
    "def test_loader_kmnist(opt, preprocess, batch_size, shuffle):\n",
    "    if opt.nc == 3:\n",
    "        preprocess = [gray_to_rgb] + preprocess\n",
    "    dataset_kmnist = dset.KMNIST(\n",
    "        root=opt.dataroot,\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=transforms.Compose([\n",
    "            transforms.Resize((opt.imageSize, opt.imageSize)),\n",
    "            transforms.ToTensor(),\n",
    "        ] + preprocess),\n",
    "    )\n",
    "    test_loader_kmnist = data.DataLoader(\n",
    "        dataset_kmnist,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=int(opt.workers),\n",
    "    )\n",
    "    return test_loader_kmnist\n",
    "\n",
    "    \n",
    "def test_loader_omniglot(opt, preprocess, batch_size, shuffle):\n",
    "    if opt.nc == 3:\n",
    "        preprocess = [gray_to_rgb] + preprocess\n",
    "    dataset_omniglot = dset.Omniglot(\n",
    "        root=opt.dataroot, \n",
    "        background=False,\n",
    "        download=True,\n",
    "        transform=transforms.Compose([\n",
    "            transforms.Resize((opt.imageSize, opt.imageSize)),\n",
    "            transforms.ToTensor(),\n",
    "        ] + preprocess),\n",
    "    )\n",
    "    test_loader_omniglot = data.DataLoader(\n",
    "        dataset_omniglot,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=int(opt.workers),\n",
    "    )\n",
    "    return test_loader_omniglot\n",
    "\n",
    "    \n",
    "def test_loader_notmnist(opt, preprocess, batch_size, shuffle):\n",
    "    if opt.nc == 1:\n",
    "        preprocess = [rgb_to_gray] + preprocess\n",
    "    class notMNIST(data.Dataset):\n",
    "        def __init__(self, db_path, transform=None):\n",
    "            super(notMNIST, self).__init__()\n",
    "            self.db_path = db_path\n",
    "            self.total_path = []\n",
    "            alphabets = os.listdir(self.db_path)\n",
    "            for alphabet in alphabets:\n",
    "                path = self.db_path + '/' + alphabet\n",
    "                elements = os.listdir(path)\n",
    "                self.total_path += [path + '/' + element for element in elements]\n",
    "            self.transform = transform\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.total_path)\n",
    "\n",
    "        def __getitem__(self, index):\n",
    "            current_path = self.total_path[index]\n",
    "            img = cv2.imread(current_path)\n",
    "            img = Image.fromarray(img)\n",
    "            img = self.transform(img)\n",
    "            return img\n",
    "\n",
    "    transform=transforms.Compose([\n",
    "        transforms.Resize((opt.imageSize, opt.imageSize)),\n",
    "        transforms.ToTensor(),\n",
    "    ] + preprocess)\n",
    "\n",
    "    notmnist = notMNIST('../../data/notMNIST_small/', transform=transform)\n",
    "    test_loader_notmnist = data.DataLoader(\n",
    "        notmnist,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=0,\n",
    "    )\n",
    "    return test_loader_notmnist\n",
    "\n",
    "    \n",
    "def test_loader_trafficsign(opt, preprocess, batch_size, shuffle):\n",
    "    if opt.nc == 1:\n",
    "        preprocess = [rgb_to_gray] + preprocess\n",
    "    class trafficsign(data.Dataset):\n",
    "        def __init__(self, db_path, transform=None):\n",
    "            super(trafficsign, self).__init__()\n",
    "            self.db_path = db_path\n",
    "            elements = os.listdir(self.db_path)\n",
    "            self.total_path = [self.db_path + '/' + element for element in elements]\n",
    "            self.transform = transform\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.total_path)\n",
    "\n",
    "        def __getitem__(self, index):\n",
    "            current_path = self.total_path[index]\n",
    "            img = cv2.imread(current_path)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            img = Image.fromarray(img)\n",
    "            img = self.transform(img)\n",
    "            return img\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((opt.imageSize, opt.imageSize)),\n",
    "        transforms.ToTensor(),\n",
    "    ] + preprocess)\n",
    "\n",
    "    ts = trafficsign('../../data/GTSRB_Final_Test_Images/Final_Test/Images', transform=transform)\n",
    "    test_loader_trafficsign = data.DataLoader(\n",
    "        ts,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=0,\n",
    "    )\n",
    "    return test_loader_trafficsign\n",
    "\n",
    "    \n",
    "def test_loader_noise(opt, preprocess, batch_size, shuffle):\n",
    "    class Noise(data.Dataset):\n",
    "        def __init__(self, number=10000, transform=None):\n",
    "            super(Noise, self).__init__()\n",
    "            self.transform = transform\n",
    "            self.number = number\n",
    "            self.total_data = np.random.randint(0, 256, (self.number, opt.nc, 32, 32))\n",
    "\n",
    "        def __len__(self):\n",
    "            return self.number\n",
    "\n",
    "        def __getitem__(self, index):\n",
    "            array = torch.tensor(self.total_data[index] / 255).float()\n",
    "            return self.transform(array)\n",
    "\n",
    "    transform = transforms.Compose(preprocess)\n",
    "\n",
    "    noise = Noise(transform=transform)\n",
    "    test_loader_noise = data.DataLoader(\n",
    "        noise,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=0,\n",
    "    )\n",
    "    return test_loader_noise\n",
    "\n",
    "    \n",
    "def test_loader_constant(opt, preprocess, batch_size, shuffle):\n",
    "    class Constant(data.Dataset):\n",
    "        def __init__(self, number=10000, transform=None):\n",
    "            super(Constant, self).__init__()\n",
    "            self.number = number\n",
    "            self.total_data = torch.randint(0, 256, (self.number, opt.nc, 1, 1))\n",
    "            self.transform = transform\n",
    "\n",
    "        def __len__(self):\n",
    "            return self.number\n",
    "\n",
    "        def __getitem__(self, index):\n",
    "            data = self.total_data[index].float()\n",
    "            data = data.repeat(32 * 32, 1, 1).reshape((-1, 32, 32)) / 255\n",
    "            return self.transform(data)\n",
    "\n",
    "    transform = transforms.Compose(preprocess)\n",
    "\n",
    "    constant = Constant(transform=transform)\n",
    "    test_loader_constant = data.DataLoader(\n",
    "        constant,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=0,\n",
    "    )\n",
    "    return test_loader_constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {'cifar10': ['svd28', 'dct28', 'gb', '20p', '4px'], 'fmnist': ['svd20', 'svd24', 'dct28', 'gb', '20p', '4px']}\n",
    "testlists = {'cifar10': ['svhn', 'celeba', 'lsun', 'mnist', 'fmnist', 'notmnist', 'noise', 'constant'],\n",
    "           'fmnist': ['cifar10', 'svhn', 'celeba', 'lsun', 'mnist', 'notmnist', 'noise', 'constant']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "def aurocTest(train_dist, blurtype, testlist, plot=True):\n",
    "    id_testloader = TEST_loader(train_dist, train_dist)\n",
    "    if train_dist == 'cifar10':\n",
    "        channel = 3\n",
    "    elif train_dist == 'fmnist':\n",
    "        channel = 1   \n",
    "    net = ResNet345(channel)\n",
    "    net2 = ResNet342(channel)\n",
    "    net = net.to(device)\n",
    "    net2 = net2.to(device)    \n",
    "\n",
    "    netPATH = f'saved_models/net_{train_dist}_{blurtype}.ckpt'\n",
    "    net2PATH = f'saved_models/net2_{train_dist}_{blurtype}.ckpt'\n",
    "\n",
    "    netckpt = torch.load(netPATH)\n",
    "    net2ckpt = torch.load(net2PATH)\n",
    "\n",
    "    net.load_state_dict(netckpt)\n",
    "    net2.load_state_dict(net2ckpt)\n",
    "\n",
    "    net.eval()\n",
    "    net2.eval()\n",
    "    \n",
    "    id_loss = np.zeros((100, 10))\n",
    "    for batch_idx, (inputs, targets) in enumerate(id_testloader):\n",
    "        inputs = inputs.to(device)\n",
    "#         inputs = Variable(inputs, requires_grad=True)\n",
    "        outputs = net(inputs)\n",
    "        outputs2, _ = net2(inputs)\n",
    "        id_loss[batch_idx] = torch.sum((outputs - outputs2)**2, axis=-1).detach().cpu().numpy()\n",
    "        if batch_idx == 99:\n",
    "            break\n",
    "    id_loss = id_loss.reshape(-1)  \n",
    "    \n",
    "    for oodtype in testlist:      \n",
    "        ood_testloader = TEST_loader(train_dist, oodtype)\n",
    "\n",
    "        ood_loss = np.zeros((100, 10))\n",
    "        for batch_idx, inputs in enumerate(ood_testloader):\n",
    "            try:\n",
    "                inputs, _ = inputs\n",
    "            except:\n",
    "                pass\n",
    "            inputs = inputs.to(device)\n",
    "#             inputs = Variable(inputs, requires_grad=True)\n",
    "            outputs = net(inputs)\n",
    "            outputs2, _ = net2(inputs)\n",
    "            ood_loss[batch_idx] = torch.sum((outputs - outputs2)**2, axis=-1).detach().cpu().numpy()\n",
    "            if batch_idx == 99:\n",
    "                break\n",
    "        ood_loss = ood_loss.reshape(-1)    \n",
    "        if plot:\n",
    "            plot_hist(id_loss, ood_loss, labels=[train_dist, oodtype])\n",
    "        auroc = AUROC(id_loss, ood_loss, labels=[train_dist, oodtype], plot=plot)\n",
    "        print(f'{train_dist}_{blurtype}_{oodtype} : {auroc:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID_blurtype_OOD : aurocScore\n",
      "Files already downloaded and verified\n",
      "Downloading http://ufldl.stanford.edu/housenumbers/test_32x32.mat to ../data\\test_32x32.mat\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f46d6173341f49c48cab3b4daeedc614",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-4cd00d240533>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[0maurocTest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestlists\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-4-a6c09bb84298>\u001b[0m in \u001b[0;36maurocTest\u001b[1;34m(train_dist, blurtype, testlist, plot)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0moodtype\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtestlist\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m         \u001b[0mood_testloader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTEST_loader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moodtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[0mood_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-f11a740e1952>\u001b[0m in \u001b[0;36mTEST_loader\u001b[1;34m(train_dist, target_dist, batch_size, shuffle)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mtarget_dist\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'svhn'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mtest_loader_svhn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreprocess3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mtarget_dist\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'celeba'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-f11a740e1952>\u001b[0m in \u001b[0;36mtest_loader_svhn\u001b[1;34m(opt, preprocess, batch_size, shuffle)\u001b[0m\n\u001b[0;32m    182\u001b[0m             \u001b[0mtransforms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mResize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimageSize\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimageSize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m             \u001b[0mtransforms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 184\u001b[1;33m         ] + preprocess),\n\u001b[0m\u001b[0;32m    185\u001b[0m     )\n\u001b[0;32m    186\u001b[0m     test_loader_svhn = data.DataLoader(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\torch14\\lib\\site-packages\\torchvision\\datasets\\svhn.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, root, split, transform, target_transform, download)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdownload\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_integrity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\torch14\\lib\\site-packages\\torchvision\\datasets\\svhn.py\u001b[0m in \u001b[0;36mdownload\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdownload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[0mmd5\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m         \u001b[0mdownload_url\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmd5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\torch14\\lib\\site-packages\\torchvision\\datasets\\utils.py\u001b[0m in \u001b[0;36mdownload_url\u001b[1;34m(url, root, filename, md5)\u001b[0m\n\u001b[0;32m     70\u001b[0m             urllib.request.urlretrieve(\n\u001b[0;32m     71\u001b[0m                 \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfpath\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m                 \u001b[0mreporthook\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgen_bar_updater\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m             )\n\u001b[0;32m     74\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mURLError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# type: ignore[attr-defined]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\torch14\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[1;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 277\u001b[1;33m                 \u001b[0mblock\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    278\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mblock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    279\u001b[0m                     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\torch14\\lib\\http\\client.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    461\u001b[0m             \u001b[1;31m# Amount is given, implement using readinto\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    462\u001b[0m             \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 463\u001b[1;33m             \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    464\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    465\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\torch14\\lib\\http\\client.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    505\u001b[0m         \u001b[1;31m# connection, and the user is reading more bytes than will be provided\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    506\u001b[0m         \u001b[1;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 507\u001b[1;33m         \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    508\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    509\u001b[0m             \u001b[1;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\torch14\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    584\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('ID_blurtype_OOD : aurocScore')\n",
    "for k in models:\n",
    "    for b in models[k]:\n",
    "        aurocTest(k, b, testlists[k], plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ycy",
   "language": "python",
   "name": "ycy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
